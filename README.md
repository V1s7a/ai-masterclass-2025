# Artificial Intelligence Masterclass + ChatGPT Prize 2025

### Resources
Section 1. Introduction
The Full World Model https://sds.courses/cm/AIMasterclass-worldModel 



Additional Reading:

The Full World Model Article https://worldmodels.github.io/

The Best Resource on Evolution Strategies http://blog.otoro.net/2017/10/29/visual-evolution-strategies/



Section 2. Artificial Neural Networks (ANN)
Additional Reading:

Yann LeCun et al., 1998, Efficient BackProp http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf

By Xavier Glorot et al., 2011, Deep sparse rectifier neural networks http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf

CrossValidated, 2015, A list of cost functions used in neural networks, alongside applications http://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications

Andrew Trask, 2015, A Neural Network in 13 lines of Python (Part 2 - Gradient Descent) https://iamtrask.github.io/2015/07/27/python-network-part2/

Michael Nielsen, 2015, Neural Networks and Deep Learning http://neuralnetworksanddeeplearning.com/chap2.html



Section 3. Convolutional Neural Networks (CNN)
Additional Reading:

Yann LeCun et al., 1998, Gradient-Based Learning Applied to Document Recognition http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf

Jianxin Wu, 2017, Introduction to Convolutional Neural Networks http://cs.nju.edu.cn/wujx/paper/CNN.pdf

C.-C. Jay Kuo, 2016, Understanding Convolutional Neural Networks with A Mathematical Model https://arxiv.org/pdf/1609.04112.pdf

Kaiming He et al., 2015, Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification https://arxiv.org/pdf/1502.01852.pdf

Dominik Scherer et al., 2010, Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf

Adit Deshpande, 2016, The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3) https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html

Rob DiPietro, 2016, A Friendly Introduction to Cross-Entropy Loss https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/

Peter Roelants, 2016, How to implement a neural network Intermezzo 2 http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/



Section 4. AutoEncoder (AE)
Additional Reading:

Malte Skarupke, 2016, Neural Networks Are Impressively Good At Compression https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/

Francois Chollet, 2016, Building Autoencoders in Keras https://blog.keras.io/building-autoencoders-in-keras.html

Chris McCormick, 2014, Deep Learning Tutorial - Sparse Autoencoder http://mccormickml.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder/

Eric Wilkinson, 2014, Deep Learning: Sparse Autoencoders http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders

Alireza Makhzani, 2014, k-Sparse Autoencoders https://arxiv.org/pdf/1312.5663.pdf

Pascal Vincent, 2008, Extracting and Composing Robust Features with Denoising Autoencoders http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf

Salah Rifai, 2011, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf

Pascal Vincent, 2010, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf

Geoffrey Hinton, 2006, Reducing the Dimensionality of Data with Neural Networks https://www.cs.toronto.edu/~hinton/science.pdf



Section 5. Variational AutoEncoder (VAE)
Additional Reading:

Irhum Shafkat, 2018, Intuitively Understanding Variational Autoencoders https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf

Diederik P. Kingma and Max Welling, 2014, Auto-Encoding Variational Bayes https://arxiv.org/pdf/1312.6114.pdf

Francois Chollet, 2016, Building Autoencoders in Keras https://blog.keras.io/building-autoencoders-in-keras.html

Chris McCormick, 2014, Deep Learning Tutorial - Sparse Autoencoder http://mccormickml.com/2014/05/30/deep-learning-tutorial-sparse-autoencoder/

Eric Wilkinson, 2014, Deep Learning: Sparse Autoencoders http://www.ericlwilkinson.com/blog/2014/11/19/deep-learning-sparse-autoencoders

Alireza Makhzani et al., 2014, k-Sparse Autoencoders k-Sparse Autoencoders https://arxiv.org/pdf/1312.5663.pdf

Pascal Vincent et al., 2008, Extracting and Composing Robust Features with Denoising Autoencoders http://www.cs.toronto.edu/~larocheh/publications/icml-2008-denoising-autoencoders.pdf

Salah Rifai et al., 2011, Contractive Auto-Encoders: Explicit Invariance During Feature Extraction http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Rifai_455.pdf

Pascal Vincent et al., 2010, Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf

Geoffrey Hinton et al., 2006, Reducing the Dimensionality of Data with Neural Networkshttps://www.cs.toronto.edu/~hinton/science.pdf 



Section 7. Recurrent Neural Networks (RNN)
Additional Reading:

Oscar Sharp & Benjamin, 2016, Sunspring https://arstechnica.com/the-multiverse/2016/06/an-ai-wrote-this-movie-and-its-strangely-moving/

Sepp (Josef) Hochreiter, 1991, Untersuchungen zu dynamischen neuronalen Netzen http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf

Yoshua Bengio, 1994, Learning Long-Term Dependencies with Gradient Descent is Difficult http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf

Razvan Pascanu, 2013, On the difficulty of training recurrent neural networks http://www.jmlr.org/proceedings/papers/v28/pascanu13.pdf

Sepp Hochreiter & Jurgen Schmidhuber, 1997, Long Short-Term Memory http://www.bioinf.jku.at/publications/older/2604.pdf

Christopher Olah, 2015, Understanding LSTM Networks http://colah.github.io/posts/2015-08-Understanding-LSTMs/

Shi Yan, 2016, Understanding LSTM and its diagrams https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714

Andrej Karpathy, 2015, The Unreasonable Effectiveness of Recurrent Neural Networks http://karpathy.github.io/2015/05/21/rnn-effectiveness/

Andrej Karpathy, 2015, Visualizing and Understanding Recurrent Networks https://arxiv.org/pdf/1506.02078.pdf

Klaus Greff, 2015, LSTM: A Search Space Odyssey https://arxiv.org/pdf/1503.04069.pdf

Xavier Glorot, 2011, Deep sparse rectifier neural networks http://jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf



Section 9. Reinforcement Learning
Additional Reading:

Arthur Juliani, 2016, Simple Reinforcement Learning with Tensorflow (10 Parts) https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0

Richard Sutton et al., 1998, Reinforcement Learning I: Introduction http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.7692

Richard Bellman, 1954, The Theory of Dynamic Programming https://www.rand.org/content/dam/rand/pubs/papers/2008/P550.pdf

D. J. White, 1993, A Survey of Applications of Markov Decision Processes http://www.cs.uml.edu/ecg/uploads/AIfall14/MDPApplications3.pdf

Martijn van Otterlo, 2009, Markov Decision Processes: Concepts and Algorithms https://pdfs.semanticscholar.org/968b/ab782e52faf0f7957ca0f38b9e9078454afe.pdf

Richard Sutton, 1988, Learning to Predict by the Methods of Temporal Differences https://link.springer.com/article/10.1007/BF00115009

